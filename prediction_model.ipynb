{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ajinkya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, EmotionOptions, SentimentOptions, EntitiesOptions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,table):\n",
    "#     global table\n",
    "    text = text.lower().replace('thanks','thank').replace(\"'\",'').replace('\"','').replace('!','').replace('.',' ').split()\n",
    "    text = lemmatize_text(text)\n",
    "    tp = {}\n",
    "    for i in text:\n",
    "        try:\n",
    "                try:\n",
    "                    tp[table[i].idxmax()] += max(table[i])\n",
    "#                     print(i,table[i].idxmax(),max(table[i]))\n",
    "                except:\n",
    "                    tp[table[i].idxmax()] = max(table[i])\n",
    "#                     print(i,table[i].idxmax(),max(table[i]))\n",
    "        except:\n",
    "            a='f'\n",
    "#     print(tp,text)\n",
    "    if(bool(tp) == False):\n",
    "        return 'Others'\n",
    "    return  max(tp.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction():\n",
    "    #Loading train data\n",
    "    table = pd.read_csv('table.csv',delimiter=',',index_col=0)\n",
    "    train_df = pd.read_csv('train_df.csv',delimiter=',',index_col=0)\n",
    "    train_df['pred'] = train_df['predictions']\n",
    "    train_df = pd.get_dummies(train_df, columns=['predictions'])\n",
    "    train_y = train_df['Action']\n",
    "    train_x = train_df.drop(['pred','Action','Topic','Tweet Full Text','classification_correct','Date','user id','follower count','place'],axis=1)\n",
    "    #Loading Test data\n",
    "    df = pd.read_csv('tweets_test.csv',delimiter='\\t', error_bad_lines=False)\n",
    "    \n",
    "    #Predicting Topic\n",
    "    df['predictions'] = df['full text'].apply(predict, args=(table,))\n",
    "    \n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "        version='2018-11-16',\n",
    "        iam_apikey='TQHgzc_QZ5w9hjhnYdFkWN0lrJk4AaXB4PkJXltCL-fA',\n",
    "        url='https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
    "    )\n",
    "    #Getting score\n",
    "    score = []\n",
    "    for i in df['full text']:\n",
    "        response2 = natural_language_understanding.analyze(\n",
    "            text=i,\n",
    "            features=Features(sentiment=SentimentOptions()),\n",
    "            language='en'\n",
    "        ).get_result()\n",
    "        s = json.loads(json.dumps(response2['sentiment']['document']['score']))  \n",
    "        try:\n",
    "            score.append(s)\n",
    "        except:\n",
    "            score.append(0)\n",
    "    df['score'] = score\n",
    "    #Getting Emotions\n",
    "    sadness,joy,fear,disgust,anger = [],[],[],[],[]\n",
    "    mydict = {}\n",
    "    for i in df['full text']:\n",
    "        response2 = natural_language_understanding.analyze(\n",
    "            text=i,\n",
    "            features=Features(emotion=EmotionOptions()),\n",
    "            language='en'\n",
    "        ).get_result()\n",
    "        mydict = json.loads(json.dumps(response2['emotion']['document']['emotion']))\n",
    "        sadness.append(mydict['sadness'])\n",
    "        joy.append(mydict['joy'])\n",
    "        fear.append(mydict['fear'])\n",
    "        disgust.append(mydict['disgust'])\n",
    "        anger.append(mydict['anger'])\n",
    "    df['sadness'] = sadness\n",
    "    df['joy'] = joy\n",
    "    df['fear'] = fear\n",
    "    df['disgust'] = disgust\n",
    "    df['anger'] = anger\n",
    "    \n",
    "    #Getting Location\n",
    "    location = []\n",
    "    mydict = {}\n",
    "    for i in df['full text']:\n",
    "        response2 = natural_language_understanding.analyze(\n",
    "            text=i,\n",
    "            features=Features(entities=EntitiesOptions()),\n",
    "            language='en'\n",
    "        ).get_result()\n",
    "        try:\n",
    "            mydict['location'] = (json.loads(json.dumps(response2['entities'][0]['type'])) == 'Location')\n",
    "        except:\n",
    "            mydict['location'] = 'False'\n",
    "        location.append(mydict['location'])\n",
    "    df['location'] = location\n",
    "    #One HOt Encoding\n",
    "    mydict = {\n",
    "        'Appreciation' : [],\n",
    "        'Community Action' : [],\n",
    "        'Dissatisfaction' : [],\n",
    "        'Fake News' : [],\n",
    "        'Follow up' : [],\n",
    "        'Fraud' : [],\n",
    "        'General Info' : [],\n",
    "        'Missing Person' : [],\n",
    "        'Others' : [],\n",
    "        'Query' : [],\n",
    "        'Suggestion' : [],\n",
    "        'Theft' : [],\n",
    "        'Traffic' : []\n",
    "    }\n",
    "    for index, row in df.iterrows():\n",
    "        for i in mydict:\n",
    "            if i == row['predictions']:\n",
    "                mydict[i].append(1)\n",
    "            else:\n",
    "                mydict[i].append(0)\n",
    "    df['predictions_Appreciation'] = mydict['Appreciation']\n",
    "    df['predictions_Community Action'] = mydict['Community Action']\n",
    "    df['predictions_Dissatisfaction'] = mydict['Dissatisfaction']\n",
    "    df['predictions_Fake News'] = mydict['Fake News']\n",
    "    df['predictions_Follow up'] = mydict['Follow up']       \n",
    "    df['predictions_Fraud'] = mydict['Fraud']           \n",
    "    df['predictions_General Info'] = mydict['General Info']   \n",
    "    df['predictions_Missing Person'] = mydict['Missing Person'] \n",
    "    df['predictions_Others'] = mydict['Others']         \n",
    "    df['predictions_Query'] = mydict['Query']          \n",
    "    df['predictions_Suggestion'] = mydict['Suggestion']     \n",
    "    df['predictions_Theft'] = mydict['Theft']          \n",
    "    df['predictions_Traffic'] = mydict['Traffic']\n",
    "    \n",
    "    #Predicting Action\n",
    "    test_df = df.drop(['Date','full text','predictions','user id','tweet id'],axis=1)\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(train_x, train_y)\n",
    "    df['Action'] = clf.predict(test_df)\n",
    "    df.to_csv('tweets_predictions.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
